---
title: "Faking It: How to Simulate Complex Data Generation Processes in R, Tidyverse Edition"
author: "A. Jordan Nafa"
date: 2022-04-19
year: "2022"
month: "2022/04"
summary: "An introduction to simulating cross-sectional time series data in R"
type: docs
tags:
  - R
  - tidyverse
  - statistics
  - data visualization
  - simulation
  - social science
  - time series
slug: data_sim_tutorial
bibliography: "../../../assets/bib/references.bib"
csl: "../../../assets/bib/apsa.csl"
css: "../../../assets/bib/style.css"
math: true
link-citations: yes
reading_time: false
commentable: true
highlight: true
highlight_style: "monokai"
editor_options:
  chunk_output_type: console
draft: false
header:
  caption: ''
  image: ''
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  fig.align = "center",
  collapse = TRUE
  )
```

Data simulation is easily near the top of the long list of useful skills that are seldom taught in social science graduate programs. This is unfortunate given the central role of simulation in model checking, sensitivity analysis, and developing a basic understanding of modeling assumptions and often complex relationships between the phenomena social scientists aspire to understand. My aim in this blog post is thus to provide a basic introduction to data simulation and parameter recovery in R for cross-sectional time series and non-nested data structures commonly encountered in political science and international relations.

```{r, echo=FALSE}
# Load the necessary libraries----
pacman::p_load(
  "tidyverse",
  "data.table",
  "dtplyr",
  "dagitty",
  "ggdag",
  "ggdist",
  "latex2exp",
  "kableExtra",
  install = FALSE
)

# A Function for checking parameter recovery
bias_table <- function(.ols_equation, .fe_equation, .re_equation, .data_list, .term, .truth) {
  
  # OLS Estimates for Each Dataset
  ols_models <- map_dfr(
    .x = .data_list,
    # Estimate an ols model for each dataset
    ~ lm(.ols_equation, .x) |>
      # Tidy the output
      broom::tidy() |> 
      # Filter the term of interest
      filter(term == .term) |> 
      # Transmute the required columns
      transmute(
        countries = unique(.x$Countries),
        periods = unique(.x$Periods + 1),
        N = countries*periods,
        ols_est = estimate,
        ols_bias = abs(estimate - .truth)
      )
  )
  
  # OLS Estimates with Unit FEs for Each Dataset
  fe_models <- map_dfr(
    .x = .data_list,
    # Estimate an ols model for each dataset
    ~ lm(.fe_equation, .x) |>
      # Tidy the output
      broom::tidy() |> 
      # Filter the term of interest
      filter(term == .term) |> 
      # Transmute the required columns
      transmute(
        countries = unique(.x$Countries),
        periods = unique(.x$Periods + 1),
        N = countries*periods,
        fe_est = estimate,
        fe_bias = abs(estimate - .truth)
      )
  )
  
  # Random Intercepts Models with Country REs for Each Dataset
  re_models <- map_dfr(
    .x = .data_list,
    # Estimate a random intercepts model for each dataset
    ~ lme4::lmer(.re_equation, .x) |>
      # Tidy the output
      broom.mixed::tidy() |> 
      # Filter the term of interest
      filter(term == .term) |> 
      # Transmute the required columns
      transmute(
        countries = unique(.x$Countries),
        periods = unique(.x$Periods + 1),
        N = countries*periods,
        re_est = estimate,
        re_bias = abs(estimate - .truth)
      )
  )
  
  # Combine the data frames into one
  ols_models |> 
    left_join(fe_models, by = c("countries", "periods", "N")) |> 
    left_join(re_models, by = c("countries", "periods", "N"))
}
```

## Thinking About Relationships and Dependencies

Directed Acylic Graphs (DAGs) provide a useful way for thinking about both simple and more complex relationships and dependencies among variables and it can often be helpful to graph out a structural model of the relationships you want to simulate. The `{dagitty}` and `{ggdag}` packages provide a straight forward approach to constructing DAGs directly R and I rely on them for the purposes herein [@Textor2016; @Barrett2021]. Imagine we are interested in estimating the causal effect of some time varying treatment $X_{t}$ on a continuous response $Y_{t}$ where the realized values of $X$ and $Y$ for each period $t \in \{1,2,\dots,T\}$ and unit $j \in \{1,2,\dots,J\}$ are a function of the treatment status for country $j$ at time $t-1$ and a vector of time varying confounders $Z_{t}$. The figure below, loosely adapted from @Blackwell2018, depicts such a data generation process.

```{r, collapse=TRUE}
# Load the necessary libraries
pacman::p_load(
  "tidyverse",
  "data.table",
  "dtplyr",
  "dagitty",
  "ggdag",
  "kableExtra",
  install = FALSE
)

# Specify the DAG for the data generation process
sim_dag <- dagify(
  x_t ~ x_tm1 + z_t + y_tm1,
  x_tm1 ~ x_tm2 + z_tm1 + y_tm2,
  x_t1 ~ x_t + y_t,
  z_t ~ z_tm1 + x_tm1 + y_tm1,
  z_tm1 ~ z_tm2 + x_tm2 + y_tm2,
  z_t1 ~ z_t + x_t + y_t,
  y_t ~ x_t + x_tm1 + z_t,
  y_tm1 ~ x_tm1 + z_tm1,
  y_tm2 ~ x_tm2 + z_tm2,
  coords = list(
    x = c(z_tm2 = -1, z_tm1 = 0, z_t = 1, z_t1 = 2, x_tm2 = -1, 
          x_tm1 = 0, x_t = 1, x_t1 = 2, y_tm1 = 0.25, y_t = 1.25, 
          y_tm2 = -0.25), 
    y = c(z_tm2 = 0, z_tm1 = 0, z_t = 0, z_t1 = 0, x_tm2 = 4, 
          x_tm1 = 4, x_t = 4, x_t1 = 4, y_tm1 = 2, y_t = 2, y_tm2 = 2)
  ),
  labels = c(z_tm2 = "bold(...)", x_tm2 = "bold(...)", 
             z_t1 = "bold(...)", x_t1 = "bold(...)", 
             x_tm1 = "bold(X[t - 1])", x_t = "bold(X[t])",
             z_tm1 = "bold(Z[t - 1])", z_t = "bold(Z[t])", 
             y_tm1 = "bold(Y[t - 1])", y_t = "bold(Y[t])", 
             y_tm2 = "bold(Y[t - 2])"),
  exposure = c("x_t"),
  outcome = c("y_t")
) %>% 
  # Create a tidy data frame from the DAG
  tidy_dagitty() %>% 
  # Set Node Status 
  node_status() %>% 
  # Set node adjacency
  node_ancestors(.var = "y_t") 
```

```{r, echo=FALSE, out.width="100%", out.height="100%", dpi = 300}
fig1_sim_dag <- sim_dag |>
  # Create Path-Specific colors and line types
  mutate(
    # Line type for the edges
    .edge_linetype = case_when(
      name %in% c("x_tm1", "z_t") & to %in% c("y_t", "x_t") ~ 2,
      TRUE ~ 1
    ),
    # Color for the edges
    .edge_colour = case_when(
      name == "x_t" & to == "y_t" ~ "blue",
      name %in% c("x_tm1", "z_t") & to %in% c("y_t", "x_t") ~ "red",
      TRUE ~ "black"
    )
  ) |>
  # Generate the DAG plot for figure 1
  ggplot(aes(x = x, y = y, xend = xend, yend = yend)) +
  # Add the graph edges
  geom_dag_edges(aes(edge_linetype = .edge_linetype, edge_colour = .edge_colour), edge_width = 1) +
  # Add the graph nodes
  geom_dag_node(alpha = 0) +
  # Add the graph text
  geom_dag_text(aes(label = label), parse = TRUE, size = 8, color = "black", family = "serif") +
  # Apply theme settings
  theme_dag(base_size = 16, base_family = "serif", title = element_text(size = 25)) +
  # Add labels to the plot
  labs(title = TeX(r'(Figure 1 DAG for the Contemporaneous Treatment Effect of $X_{t}$ on $Y_{t}$)', bold = T))

# Plot the figure
knitr::include_graphics(path = "figures/Figure_1_DAG_Example.jpeg")
```

There's quite a bit going on here, so let's break it down a bit. First, the solid blue line in figure 1 represents the causal path of interest $X_{t} \longrightarrow Y_{t}$. The dashed red lines indicate confounding paths--nodes that have a direct effect on both the outcome and the treatment--that need to be adjusted for to identify the contemporaneous effect of $X$ on $Y$ at each period $t$. As depicted in the DAG, the lag of the treatment $X_{t-1}$ and the vector time-varying confounders $Z_{t}$ constitute the minimum sufficient adjustment set necessary to identify the path $X_{t} \longrightarrow Y_{t}$. This is further illustrated in figure 2 below which shows the adjusted nodes in green, the causal path $X_{t} \longrightarrow Y_{t}$ in blue, and nodes we do not need to adjust for to identify the effect of $X$ on $Y$ at time $t$ in black. 

```{r, echo=FALSE, out.width="100%", out.height="100%", dpi = 300}
fig2_sim_dag <- sim_dag |>
  # Create Path-Specific colors and line types
  mutate(
    # Color for the edges
    .text_colour = case_when(
      name %in% c("x_t", "y_t") ~ "#0000FF",
      name %in% c("x_tm1", "z_t") ~ "#008B45",
      TRUE ~ "#000000"
    ),
    # Color for the edges
    .edge_colour = case_when(
      name == "x_t" & to == "y_t" ~ "#0000FF",
      name %in% c("x_tm1", "z_t") ~ "white",
      TRUE ~ "black"
    )
  ) |>
  # Generate the DAG plot for figure 2
  ggplot(aes(x = x, y = y, xend = xend, yend = yend)) +
  # Add the graph edges
  geom_dag_edges(
    aes(edge_colour = .edge_colour), 
    edge_width = 1) +
  # Add the graph nodes
  geom_dag_node(alpha = 0) +
  # Add the graph text
  geom_dag_text(
    aes(label = label, color = .text_colour), 
    parse = TRUE, size = 8, family = "serif"
  ) +
  # Apply text color settings
  scale_color_manual(
    values = c("#000000", "#0000FF", "#008B45"),
    labels = c("Unadjusted Nodes", "Identified Path", "Adjusted Nodes")
  ) +
  # Apply theme settings
  theme_dag(
    base_size = 16, 
    base_family = "serif", 
    title = element_text(size = 25), 
    legend.text = element_text(size = 18), 
    legend.position = "bottom"
  ) +
  # Add labels to the plot
  labs(title = TeX(r'(Figure 2 DAG for the Contemporaneous Effect of $X_{t}$ on $Y_{t}$ Adjusting for $X_{t-1}$ and $Z_{t}$)', bold = T)) +
  guides(color = guide_legend(
    title = "Node Status",
    title.position = "left"
  ))

# Plot the figure
knitr::include_graphics(path = "figures/Figure_2_DAG_Example.jpeg")
```

We can also more directly verify sufficiency of the set $\{X_{t-1}, Z_{t}\}$ using the `adjustmentSets` function from the `{dagitty}` package.[^1]

[^1]: This could also be obtained via {ggdag}'s `dag_adjustment_sets` or visually via `ggdag_adjustment_set`. See the [{ggdag} package site](https://ggdag.malco.io/) for more details

```{r}
# Verify the minimum adjustment using dagitty
adjustmentSets(sim_dag$dag, exposure = "x_t", outcome = "y_t")
```

While this is all rather abstract, it provides a useful way to think about potentially complex relationships between variables and how they all do or don't fit into a particular model.

## Simulating Cross-Sectional Time Series Data

Having specified our dependencies and identified the main parameter(s) we intend to recover, the effect of $X_{t}$ on $Y_{t}$ at time $t$, we can simulate a series of datasets with dimensions similar to those encountered in real world social science applications. To begin, let's specify the dimensions for each dataset as a tibble with two columns, Countries and Periods, and expand it by each possible combination. We'll then add two additional columns, a unique identifier for each dataset and its total number of observations.
 
```{r}
# Define the Dimensions for the Simulated Data
sim_dims <- tibble(
  Countries = seq(10, 85, 25),  # Number of Countries
  Periods = seq(12, 57, 15) # Number of Time Periods
  ) |>
  # Expand the data for each combinations of dimensions
  expand(Countries, Periods) |>
  # Add Total Observations and a dataset ID
  mutate(N = Countries*Periods, data_id = 1:n())

# Print the data
head(sim_dims)
```

To simulate the data for each combination of dimensions we can nest each row in the tibble by its country-period pair and then loop across performing the necessary operations via the `map` function from `{purrr}`. This approach is more computationally efficient in R than using for loops and relies primarily on the `{tidyverse}` package suite, though the difference is probably negligible for the purposes of this example. I assume the baseline data for each of the parameters follows a normal distribution with mean $\mu_{j}$ and standard deviation $\sigma_{j}$ where $\mu_{j} \sim \mathcal{N}(0, 1)$ and $\sigma_{j} \sim \mathcal{Exponential}(\lambda = 0.5)$ for each country $j$.

```{r}
# Set the rng seed to ensure values are reproducible
set.seed(123456)

sim_data <- sim_dims |> 
  # Nest the tibble by dataset
  nest(data = c(Countries, Periods)) |>
  # Simulate each of the datasets
  mutate(datasets = map(
    .x = data,
    ~ .x |>
      # Expand the data intol panel format
      expand_grid(country = 1:Countries, period = 1:Periods) |>
      # Group the data by country
      group_by(country) |> 
      # Simulate baseline values for the parameters
      mutate(
        # Baseline Values for Z
        Z_base = rnorm(n(), rnorm(1, 0, 1), rexp(1, 0.5)),
        # Baseline Values for X
        X_base = rnorm(n(), rnorm(1, 0, 1), rexp(1, 0.5)),
        # Baseline Values for Y
        Y_base = rnorm(n(), rnorm(1, 0, 1), rexp(1, 0.5)),
        # Lagged Baseline Values for Each of the Parameters
        across(
          ends_with("_base"),
          .fns = list(
            lag_1 = ~ lag(.x, n = 1L),
            lag_2 = ~ lag(.x, n = 2L)
          ),
          .names = "{.col}_{.fn}"
        ),
        # Floor periods at 3 to accomodate the dropped lags
        across(c(period, Periods), ~ .x - 2)
        ) |> 
      # Ungroup the data
      ungroup() |>
      # Drop missing values
      drop_na()
  )) |> 
  # Unnest the dimensions columns
  unnest(cols = data)

# Print the first dataset
head(sim_data$datasets[[1]])
```

Now that we have a dataset with baseline values of $X$, $Z$, $Y$, and their lags for each of combination of dimensions, we can simulate "true" coefficient values for each of the vectors at time $t$ and $t - 1$. In a real simulation study you would likely want to assign fixed values to the nuisance parameters and vary the magnitude of the effect of the treatment on the response but for the purpose here I'm just going to generate a vector of twenty random numbers from a uniform distribution.

```{r}
# Simulate a vector of "true" coefficient values for each parameter
beta <- runif(20, -5, 5)
```

We can then simulate the parameters of interest by simply following the paths on the DAG in figure 1 for each parameter, working our way from left to right. Alternatively, we could pass each of the parameters to the `ggdag::ggdag_parents` or `dagitty::parents` convenience functions to identify the parents of each node and specify the relationships accordingly.

```{r}
# Character vector specifying nodes on the graph
params <- c("x_tm1", "z_tm1", "y_tm1", "x_t", "z_t", "y_t")

# Get the parents for each node
parent_nodes <- map(.x = params, ~ parents(sim_dag$dag, .x))

# Set names for the nodes
names(parent_nodes) <- params

# Print the parents for node X[t-1]
parent_nodes$x_tm1
```

The output tells us that $X_{t-1}$ is a function of $X_{t-2}$, $Y_{t-2}$, and $Z_{t-1}$ which we can express as its own equation along the lines of $X_{t-1} \sim \beta_{0} + \beta_{1}X_{t-2} + \beta_{2}Y_{t-2} + \beta_{3}Z_{t-1}$. We can repeat this process for each of the relationships defined in the DAG as shown in the code below.

```{r}
# Simulate the model dependencies based on the DAG
sim_df <- sim_data |> 
  mutate(
    datasets = map(
      .x = datasets,
      ~ .x |>
        transmute(
          # Identifiers
          across(Countries:period, ~ .x),
          # Data for Y[t-2] ~ Y_Lag_2_Base + 5.914*Z[t-2] + -3.570*X[t-2]
          Y_Lag_2 = Y_base_lag_2 + 5.914*Z_base_lag_2 + -3.570*X_base_lag_2,
          # Data for Z[t-1] ~ Z_Lag_Base + X[t-2]*beta[1] + Y[t-2]*beta[2] + Z[t-2]*beta[3]
          Z_Lag = Z_base_lag_1 + Z_base_lag_2*beta[1] + Y_Lag_2*beta[2] + X_base_lag_2*beta[3],
          # Data for X[t-1] ~ X_Lag_Base + X[t-2]*beta[4] + Y[t-2]*beta[5] + Z[t-2]*beta[6]
          X_Lag = X_base_lag_1 + X_base_lag_2*beta[3] + Y_Lag_2*beta[5] + Z_Lag*beta[6],
          # Data for Y[t-1] ~ Y_Lag_Base + X[t-1]*beta[7] + Z[t-1]*beta[8] + Noise
          Y_Lag = Y_base_lag_1 + X_Lag*beta[7] + Z_Lag*beta[8] + rnorm(n(), 0, 1),
          # Data for Z[t] ~ Z_Base + X[t-1]*beta[9] + Z[t-1]*beta[10] + Y[t-1]*beta[11] + Noise
          Z = Z_base + X_Lag*beta[9] + Z_Lag*beta[10] + Y_Lag*beta[11] + rnorm(n(), 0, 1),
          # Data for X[t] ~ X_Base + X[t-1]*beta[12] + Z[t]*beta[13] + Y[t-1]*beta[14] + Noise
          X = X_base + X_Lag*beta[12] + Z*beta[13] + Y_Lag*beta[14] + rnorm(n(), 0, 1),
          # Data for Y[t] ~ Y_Base + X[t]*beta[15] + X[t-1]*beta[16] + Z[t]*beta[17] + Noise
          Y = Y_base + X*beta[15] + X_Lag*beta[16] + Z*beta[17] + rnorm(n(), 0, 1)
        )
    ))

# Print the first dataset
head(sim_df$datasets[[1]])
```

Now that we've simulated the data, let's check the true parameter values for $\beta_{15}$ so we know what we're aiming for.

```{r}
# Check the parameter values for Y
beta[15:17]
```

If everything is specified correctly, the data generation process for $Y_{t}$ should be $\beta_{0} -2.30 \cdot X_{t} + 0.21\cdot X_{t-1} + 0.89\cdot Z_{t} + \epsilon$ where the causal effect of $X$ on $Y$ at time $t$ is approximately $-2.30$. The function `bias_table` simply loops across each of the simulated data frames, estimating the specified models, and returning a tibble containing the estimates and absolute bias. As table 1 illustrates, we're able to recover the true parameter value suggesting everything worked as intended.

```{r}
param_recovery <- bias_table(
  .ols_equation = Y ~ X + X_Lag + Z, 
  .fe_equation = Y ~ X + X_Lag + Z + as.factor(country), 
  .re_equation = Y ~ X + X_Lag + Z + (1 | country),
  .data_list = sim_df$datasets, 
  .term = "X", 
  .truth = beta[15]
)
```

```{r, echo=FALSE}
kable(
  param_recovery,
  col.names = c("Countries", "Periods", "N", "Estimate", "Bias", "Estimate", "Bias", "Estimate", "Bias"),
  align = "c",
  digits = 4
  ) %>% 
  kable_classic(html_font = "serif") %>% 
  add_header_above(c("Dimensions" = 3, "Complete Pooling" = 2, "No Pooling" = 2, "Partial Pooling" = 2)) %>% 
  add_header_above(c(" " = 3, "Parameter Estimates" = 6)) %>% 
  footnote(number = "Bias is the difference between the true value of the parameter and the estimated coefficient for each dataset in absolute terms. The true parameter value for the effect of $X_{t}$ on $Y_{t}$ is approximately -2.3039.")
```

And there you have it, a basic introduction to simulating data and parameter recovery using DAGs and the tidyverse. I'll probably do a follow-up post in a few weeks that covers simulating discrete parameters such as binary treatments and responses. You can find all of the code not shown for the sake of brevity [on my github](https://github.com/ajnafa/ajnafa-website/blob/main/content/blog/04-19-2022-data-sim-tutorial/index.Rmd)

# References
