---
title: "An Introduction to Being Less Wrong: Bayesian Model Averaged Marginal Effects"
author: "A. Jordan Nafa"
date: 2022-05-24
year: "2022"
month: "2022/05"
summary: "An Introduction to Bayesian Model Averaged Marginal Effects (BMAMEs) with `{brms}` and `{marginaleffects}`"
type: docs
images: 
- /blog/2022/05/24/bma-ames/assets/figures/Figure_2_BAMEs_Female_Combatants.jpeg
tags:
  - R
  - brms
  - Stan
  - Bayesian
  - tidyverse
  - statistics
  - data visualization
  - model averaging
  - marginal effects
  - political science
slug: bma-ames
bibliography: "../../../assets/bib/references.bib"
csl: "../../../assets/bib/apsa.csl"
css: "style.css"
math: true
link-citations: yes
reading_time: false
commentable: true
highlight: true
highlight_style: "monokai"
editor_options:
  chunk_output_type: console
draft: false
header:
  caption: ''
  image: ''
---

  <link rel="stylesheet" href="style.css" type="text/css" />


<style type="text/css">
.side-by-side-code .first-block {
    padding-right: 10px;
}
.side-by-side-code {
    font-size: 0.8em;
}
</style>
<p>As George E. P. Box famously posed the problem, “all models are wrong, but some models are useful” and thus the fundamental question practitioners of applied statistics are faced with is <em>how wrong can we be less wrong?</em> This question tends to be a recurring theme of my own dissertation research, which makes extensive use of various approaches to Bayesian Model Averaging (BMA) in pursuit of this end one of which is Bayesian Model Averaged Marginal Effects (BMAMEs). Thanks to Vincent Arel-Bundock’s help the <code>{marginaleffects}</code> R package now has an implementation for estimating BMAMEs that works with any model fit with <code>{brms}</code> <span class="citation">(<a href="#ref-ArelBundock2022" role="doc-biblioref">Arel-Bundock 2022</a>; <a href="#ref-Buerkner2017" role="doc-biblioref">Bürkner 2017</a>, <a href="#ref-Buerkner2018" role="doc-biblioref">2018</a>)</span>. This blog post is designed to serve as a general demonstration of how to propagate uncertainty in the process of model selection forward when estimating average marginal effects for Bayesian regression models.</p>
<div class="a">
<p>The first section provides a brief overview of BMA in the context of political science, its advantages and shortcomings, and recently proposed alternative approaches that may help overcome some of the issues with traditional BMA <span class="citation">(e.g., <a href="#ref-Yao2018" role="doc-biblioref">Yao et al. 2018</a>)</span>. In the second half of the post, I turn to a demonstration and comparison of each of the approaches to BMAMEs supported by <code>{marginaleffects}</code> based on an applied example from the second chapter of my dissertation which examines how the participation of women in combat during civil wars shapes the transition of former rebel groups to political parties at wars end <span class="citation">(<a href="#ref-Nafa2022" role="doc-biblioref">Nafa and Roark 2022</a>)</span>. While no approach to estimation is perfect, BMA and its variants provide a principled, relatively robust way to propagate uncertainty in quantities typically of interest to researchers in the social sciences and are capable of helping us avoid unnecessary dichotomies that are, in most cases, false.</p>
</div>
<div id="approaches-to-bayesian-model-averaging-for-political-research" class="section level2">
<h2>Approaches to Bayesian Model Averaging for Political Research</h2>
<p>Despite it’s introduction to the discipline more than two decades ago by <span class="citation">Bartels (<a href="#ref-Bartels1997" role="doc-biblioref">1997</a>)</span>, applications of BMA in political science remain rare and are largely confined to the topic of political methodology <span class="citation">(<a href="#ref-Cranmer2015" role="doc-biblioref">Cranmer, Rice, and Siverson 2015</a>; <a href="#ref-Juhl2019" role="doc-biblioref">Juhl 2019</a>; <a href="#ref-Montgomery2010" role="doc-biblioref">Montgomery and Nyhan 2010</a>)</span>. This presents something of a problem given the numerous studies demonstrating the dangers of tossing a large number of likely correlated predictors into a regression model <span class="citation">(<a href="#ref-Achen2005" role="doc-biblioref">Achen 2005</a>; <a href="#ref-Clarke2005" role="doc-biblioref">Clarke 2005</a>, <a href="#ref-Clarke2009" role="doc-biblioref">2009</a>; <a href="#ref-Montgomery2018" role="doc-biblioref">Montgomery, Nyhan, and Torres 2018</a>)</span>; the reality that if we wish to adjudicate between two or more competing theories comparing coefficients in a single model is generally insufficient to accomplish such a task <span class="citation">(<a href="#ref-Clarke2007" role="doc-biblioref">Clarke 2007</a>; <a href="#ref-Hollenbach2020" role="doc-biblioref">Hollenbach and Montgomery 2020</a>; <a href="#ref-Imai2011" role="doc-biblioref">Imai and Tingley 2011</a>)</span>; and the difficulty of assessing what we truly known about political phenomenon that results from an obsession with statistical significance rather than predictive validity <span class="citation">(<a href="#ref-Cranmer2017" role="doc-biblioref">Cranmer and Desmarais 2017</a>; <a href="#ref-Schrodt2014" role="doc-biblioref">Schrodt 2014</a>; <a href="#ref-Ward2010" role="doc-biblioref">Ward, Greenhill, and Bakke 2010</a>)</span>. Model averaging provides a natural and intuitive way of resolving many of these issues and tends to have a substantially lower false positive rate than alternative approaches <span class="citation">(<a href="#ref-Pluemper2018" role="doc-biblioref">Plümper and Traunmüller 2018</a>)</span>.</p>
<div id="bayesian-model-averaging" class="section level3">
<h3>Bayesian Model Averaging</h3>
<p>In contrast to Montgomery and Nyhan’s <span class="citation">(<a href="#ref-Montgomery2010" role="doc-biblioref">2010</a>)</span> suggestion that “BMA is best used as a subsequent robustness check to show that our inferences are not overly sensitive to plausible variations in model specification” (266), model averaging can and should be used as far more than a robustness check. Consider a case in which we have a set of <span class="math inline">\(\mathcal{M}\)</span> possible models, each of which characterizes a given theoretical hypothesis about the process that generated the observed data <span class="math inline">\(y\)</span>. In the case of only two models competing models, the posterior odds of a given model <span class="math inline">\(i\)</span> relative to an alternative <span class="math inline">\(k\)</span> is</p>
<p><span class="math display">\[
\underbrace{\frac{p(\mathcal{M}_{i} \, | \, y)}{p(\mathcal{M}_{k} \, | \, y)}}_{\text{Posterior Odds}} = \underbrace{\frac{p(y \, | \, \mathcal{M}_{i})}{p(y \, | \, \mathcal{M}_{k})}}_{\text{Bayes Factor}} \times \underbrace{\frac{\pi(\mathcal{M}_{i})}{\pi(\mathcal{M}_{k})}}_{\text{Model Prior}}
\]</span>
where <span class="math inline">\(p(y \,|\, \mathcal{M})\)</span> is the marginal likelihood of the observed data under each of the candidate models and may be expressed as</p>
<p><span class="math display">\[
\underbrace{p(y \,|\, \mathcal{M})}_{\text{Marginal Likelihood}} = \int\underbrace{p(y \,|\, \theta,\, \mathcal{M})}_{\text{Likelihood}} \, \underbrace{\pi(\theta \,|\, \mathcal{M})}_{\text{Prior}}d\theta
\]</span></p>
<div class="a">
<p>In practice, for all but the simplest models deriving this integral analytically is computationally intractable and it is necessary to rely on algorithmic approximations such as bridge sampling <span class="citation">(<a href="#ref-Gelman1998" role="doc-biblioref">Gelman and Meng 1998</a>; <a href="#ref-Gronau2017" role="doc-biblioref">Gronau et al. 2017</a>; <a href="#ref-Wang2020" role="doc-biblioref">Wang, Jones, and Meng 2020</a>)</span>, some of the limits of which I discuss in further detail below. Extending this to the setting in which <span class="math inline">\(\mathcal{M_{k}}\)</span> is a set of plausible competing models of size <span class="math inline">\(k &gt; 1\)</span>, the posterior model probability of the <span class="math inline">\(i^{th}\)</span> model relative to the set of alternatives <span class="math inline">\(\mathcal{M_{k}}\)</span> is</p>
</div>
<p><span class="math display">\[
\Pr(\mathcal{M}_{i} \,|\, y) = \frac{p(y \, | \, \mathcal{M}_{i}) \, \cdot \, \pi(\mathcal{M}_{i})}{\displaystyle\sum^{\mathcal{m}}_{k=1} p(y \, | \, \mathcal{M}_{k}) \, \cdot \, \pi(\mathcal{M}_{k})}
\]</span></p>
<p>By repeating this calculation for each model <span class="math inline">\(i \in \{1,2,\dots,\mathcal{M}\}\)</span>, we obtain a vector of weights of length <span class="math inline">\(\mathcal{M}\)</span>. We can then take <span class="math inline">\(n\)</span> random draws from the posterior predictive distribution of each model where <span class="math inline">\(n\)</span> is proportional to its respective posterior model probability weight, resulting in a weighted average of the posterior distributions that accounts for uncertainty in the model selection process.</p>
</div>
<div id="issues-with-bma-and-alternative-approaches-to-weighting" class="section level3">
<h3>Issues with BMA and Alternative Approaches to Weighting</h3>
<p>The traditional approach to BMA is not without problems, at least three of which are notable and warrant some discussion. First, traditional BMA rests upon a closed-<span class="math inline">\(\mathcal{M}\)</span> assumption–that is, the “true” model is among those under consideration in the set of candidate models <span class="math inline">\(\mathcal{M}\)</span>. In the open-<span class="math inline">\(\mathcal{M}\)</span> setting which arises when the “true” model is not among those under consideration, as will generally be the case in any social science application, the traditional approach to BMA is flawed and estimated effects based on posterior probability weights are likely to be biased <span class="citation">(<a href="#ref-Hollenbach2020" role="doc-biblioref">Hollenbach and Montgomery 2020</a>; <a href="#ref-Yao2018" role="doc-biblioref">Yao et al. 2018</a>)</span>. The extent to which this presents a problem depends on the nature of the research question and the inferential goals of the researcher, but it is generally advisable to undertake some form of sensitivity analysis to ensure the analyst’s substantive conclusions are not sensitive to violations of the closed-<span class="math inline">\(\mathcal{M}\)</span> assumption.</p>
<div class="a">
<p>The second area of concern stems from the fact that posterior model probability weights may be highly sensitive to priors on parameters, model priors, or both. It is by this point common knowledge that specifying flat priors on the parameters of a model will tend to bias Bayes Factors, and by extension values that depend on them, violently in favor of the null model–that is “flat” priors, which to begin with are not a mathematically defined concept, can end up being extremely informative in all the wrong ways. My perhaps controversial take on this issue is that this is a feature of applied Bayesian inference rather than a bug–<em>if you make stupid assumptions, you will end up with stupid results</em>. The solution to this problem is to think carefully about what the universe of possible effect sizes you might observe is, assign reasonable priors that constrain the parameter space, and verify that the results are robust to alternative distributional assumptions.</p>
</div>
<div class="a">
<p>The final concern relates to the possibility that the bridge sampling approximation used to derive the marginal likelihood may not be reliable. <span class="citation">Schad et al. (<a href="#ref-Schad2022" role="doc-biblioref">2022</a>)</span> provide guidelines for robust inference and sensitivity analysis based on approximate Bayes Factors, though many of the issues they highlight stem from logical inconsistencies inherent in the framework of null hypothesis significance testing that employing Bayesian methods does not somehow magically fix <span class="citation">(<a href="#ref-McShane2019" role="doc-biblioref">McShane et al. 2019</a>)</span>. As stated above, it is generally advisable to specify reasonable priors and consider alternative prior distributions. In addition to these suggestions, it is also generally advisable to estimate a distribution of the approximate marginal likelihood by running the bridge sampling algorithm multiple times for each model to ensure stability of the estimates.</p>
</div>
<div class="a">
<p>To address the flaws of traditional BMA, virtually all of which stem from it’s reliance on Bayes Factors <span class="citation">(<a href="#ref-Hollenbach2020" role="doc-biblioref">Hollenbach and Montgomery 2020</a>)</span>, in the open-<span class="math inline">\(\mathcal{M}\)</span> setting <span class="citation">Yao et al. (<a href="#ref-Yao2018" role="doc-biblioref">2018</a>)</span> propose several alternative approaches to weighting posterior predictive distributions that do not rely on posterior model probabilities and thus avoid invoking the closed-<span class="math inline">\(\mathcal{M}\)</span> assumption. Instead, these approaches rely on predictive weights based on some form of either exact or approximate cross validation and implemented in the <code>loo</code> package <span class="citation">(<a href="#ref-Piironen2016" role="doc-biblioref">Piironen and Vehtari 2016</a>; <a href="#ref-Vehtari2020" role="doc-biblioref">Vehtari et al. 2020</a>)</span>. <a href="https://projecteuclid.org/journals/bayesian-analysis/volume-13/issue-3/Using-Stacking-to-Average-Bayesian-Predictive-Distributions-with-Discussion/10.1214/17-BA1091.full">Yao et al. <span class="citation">(<a href="#ref-Yao2018" role="doc-biblioref">2018</a>)</span></a> and <a href="http://fhollenbach.org/papers/Hollenbach_Montgomery_2019_BayesianModelSelection.pdf">Hollenbach and Montgomery <span class="citation">(<a href="#ref-Hollenbach2020" role="doc-biblioref">2020</a>)</span></a> provide accessible overviews of these approaches and in the interest brevity I direct interested readers to consult those works for further details.</p>
</div>
<div class="a">
<p>Since predictive-based approaches to estimating model weights do not rely upon the closed-<span class="math inline">\(\mathcal{M}\)</span> assumption, they provide a way of either avoiding it altogether–whether this is the correct approach is fundamentally depends on the research question to be answered–or relaxing it as a robustness check on the posterior probability weights. Note that the core difference between each of these approaches to model averaging lies primarily in how the weights are estimated and the process of averaging across the posterior predictive distributions of each model by taking random draws from its posterior distribution proportional to its model weight is the same regardless of which approach is taken.</p>
</div>
</div>
</div>
<div id="bayesian-model-averaged-marginal-effects-an-applied-example" class="section level2 tabset">
<h2>Bayesian Model Averaged Marginal Effects: An Applied Example</h2>
<p>Once we’ve decided how to obtain the model weights, the <code>{marginaleffects}</code> package provides the necessary functionality to handle everything else for us thanks to the feature-rich support for various approaches to averaging across posterior distributions provided by <code>{brms}</code>’ <code>pp_average</code> function. To obtain the BMAME for a given parameter while accounting for the uncertainty in the model specifications, version 0.5.0 of <code>{marginaleffects}</code> allows users to specify the argument <code>type = "average"</code> passed to the main workhorse function for objects of class <code>brmsfit</code> and any additional arguments to be passed down to <code>pp_average</code> such as the type of weights to estimate, or alternatively a numeric vector of pre-estimated weights which is usually the more computationally efficient option and the approach I take in the applied example below.</p>
<div class="a">
<p>To demonstrate this new functionality, I draw on an example loosely adapted from part of my dissertation research in which I examine how the active participation of women in rebel groups during wartime shapes the political calculus of former combatant groups at war’s end–in short, I expect rebel groups where women participated in combat roles during wartime are <em>more likely</em> to form political parties and participate in post-conflict elections. Note that this demonstration is not comprehensive and only illustrates one aspect of how BMAMEs might be applied in practice. However, as far as <code>{marginaleffects}</code> functionality is concerned, the general workflow is identical regardless.</p>
</div>
<div id="data" class="section level3">
<h3>Data</h3>
<p>The outcome, <em>Rebel to Party Transition</em>, is a dichotomous event that takes a value of one if a rebel group both forms a political party and participates in the country’s first post-conflict elections and zero otherwise.<a href="#fn1" class="footnote-ref" id="fnref1"><sup>1</sup></a> To identify instances where women participated in combat roles on the side of rebel groups during wartime, I construct a binary indicator based on the Women in Armed Rebellion Dataset <span class="citation">(<a href="#ref-Wood2017" role="doc-biblioref">Wood and Thomas 2017</a>)</span>. Finally, I also adjust for baseline characteristics including group ideology and goals <span class="citation">(<a href="#ref-Braithwaite2019" role="doc-biblioref">Braithwaite and Cunningham 2019</a>)</span>, organizational capacity and governance structure <span class="citation">(<a href="#ref-Albert2022" role="doc-biblioref">Albert 2022</a>)</span>,<a href="#fn2" class="footnote-ref" id="fnref2"><sup>2</sup></a> and geography that may impact both whether women participate in combat on behalf of a given rebel group and whether that group transitions to a political party in the aftermath of conflict.</p>
<div class="a">
<p>Relevant for the demonstration of BMA, is the fact that WARD provides two separate indicators for the presence of female combatants, one that includes cases in which women only participated in combat as suicide bombers and a second that excludes those cases. In terms of which of these is more appropriate for the analysis at hand, theory provides little guidance. Rather than simply running the analysis on both and reporting the set of results that is more favorable, BMA provides a way of combining what are effectively two slightly different measures of the same concept into a single model averaged estimate.</p>
</div>
<pre class="r"><code>## Set Project Options----
options(
  digits = 6, # Significant figures output
  scipen = 999, # Disable scientific notation
  repos = getOption(&quot;repos&quot;)[&quot;CRAN&quot;],
  mc.cores = 12L,
  knitr.kable.NA = &#39;&#39;,
  future.globals.maxSize = 24000*1024^2
)

## Load the necessary libraries----
pacman::p_load(
  &quot;tidyverse&quot;,
  &quot;data.table&quot;,
  &quot;dtplyr&quot;,
  &quot;brms&quot;,
  &quot;kableExtra&quot;,
  &quot;furrr&quot;,
  &quot;modelsummary&quot;,
  &quot;marginaleffects&quot;,
  &quot;tidybayes&quot;,
  &quot;latex2exp&quot;,
  &quot;patchwork&quot;,
  install = FALSE
)

## Load the Pre-Processed Data for the Main Analysis----
df &lt;- read_rds(&quot;assets/Main_Project_Data.RDS&quot;)

### Model data with female combatants (including suicide bombers)----
model_df_a &lt;- df %&gt;% 
  transmute(
    # Identifiers and outcome
    across(c(row_id:no_election, rebel_gov_theta:rebel_gov_upper)),
    # Center Predictors at their mean
    across(
      c(femrebels, ethnic_id, party_affil, 
        left_ideol:idependence, africa:post_coldwar), 
    ~ ((as.integer(.x) - 1) - mean((as.integer(.x) - 1), na.rm = TRUE))
  ))

### Model data with female combatants (excluding suicide bombers)----
model_df_b &lt;- df %&gt;% 
  transmute(
    # Identifiers and outcome
    across(c(row_id:no_election, rebel_gov_theta:rebel_gov_upper)),
    # Center Predictors at their mean
    across(
      c(femrebels_exs, ethnic_id, party_affil, 
        left_ideol:idependence, africa:post_coldwar), 
      ~ ((as.integer(.x) - 1) - mean((as.integer(.x) - 1), na.rm = TRUE))
    )) %&gt;% 
  # Rename female rebels (excluding suicide bombers) to avoid later issues
  rename(femrebels = femrebels_exs)</code></pre>
</div>
<div id="priors" class="section level3">
<h3>Priors</h3>
<p>For the purposes of this example, I estimate a series of Bayesian logistic regression models whose coefficients are assigned independent normal priors with mean 0 and standard deviation <span class="math inline">\(\frac{3}{4}\)</span>. In practical terms, these are weakly to moderately informative insofar as they are agnostic about the direction of the effect but concentrate 95% of the probability mass in the range of <span class="math inline">\(\pm 1.5\)</span> on the logit scale. We’ll place a slightly wider prior from a T distribution on the overall intercept term <span class="math inline">\(\alpha\)</span> and then, since this is all fairly abstract, we can visualize the prior distributions as shown below.</p>
<p><span class="math display">\[
\begin{aligned}
y_{i} \sim&amp; Bernoulli(\text{logit}^{-1}[\theta_{i}])\\
\theta_{i} &amp;= \alpha + X_{n}\beta_{k}\\
\textit{with priors}\\
\alpha &amp;\sim \textit{Student T}(15,\, 0,\, 1) \\
\beta_{1:k} &amp;\sim \textit{Normal}(0, \, 0.75)\\
\end{aligned}
\]</span></p>
<pre class="r"><code># Specify the priors using standard brms syntax
priors_df &lt;- prior(normal(0, 0.75), class = &quot;b&quot;) +
  prior(student_t(15, 0, 1), class = &quot;b&quot;, coef = &quot;Intercept&quot;)

# Parse the brms priors object
priors_df &lt;- parse_dist(priors_df)

### Set parameter names for the facets----
prior_labels &lt;- as_labeller(
  x = c(
    &quot;normal(0, 0.75)&quot; = TeX(
      r&#39;($\beta_{k} \, \sim \, Normal(0,\, 0.75)$)&#39;, 
      output = &quot;character&quot;, 
      bold = T,
      italic = T
      ),
    &quot;student_t(15, 0, 1)&quot; = TeX(
      r&#39;($\alpha\, \sim \, Student\, T(15,\, 0,\, 1)$)&#39;, 
      output = &quot;character&quot;, 
      bold = T,
      italic = T
      )
  ), 
  default = label_parsed
)

## Plot the prior distributions
priors_plot_main %&gt;% 
  # Initiate the ggplot object
  ggplot() +
  # Facet by the prior distributions
  facet_wrap(vars(prior), scales = &quot;free&quot;, labeller = prior_labels) +
  # Add a half eye geom for the prior distributions
  stat_dist_halfeye(
    aes(dist = .dist, args = .args, fill = prior, slab_alpha = stat(pdf)),
    fill_type = &quot;segments&quot;,
    show.legend = F
  ) +
  # Flip the x and y axis
  coord_flip() + 
  # Set the fill parameter for each group
  scale_fill_viridis_d() +
  # Custom plot theme settings
  plot_theme(
    plot.margin = margin(5, 5, 5, 5, &quot;mm&quot;),
    strip_size = 18,
    xaxis_size = 22,
    yaxis_size = 22
    ) +
  # Add labels to the plot
  labs(
    x = &quot;Density&quot;,
    title = &quot;Figure 1. Priors for Logistic Regression Model Parameters&quot;,
    y = latex2exp::TeX(r&#39;(Log $\Pr(\theta_{prior})$)&#39;, bold = T)
  )</code></pre>
<p><img src="assets/figures/Figure_1_Priors.jpeg" width="100%" height="100%" style="display: block; margin: auto;" /></p>
</div>
<div id="estimation" class="section level3">
<h3>Estimation</h3>
<p>The <code>brms</code> R package provides an easy to use R interface to Stan. Here we estimate each of the models sequentially by looping over the formulas and data sets. In this example, I run each model with six Markov chains in parallel for 8000 iterations per chain discarding the first 3,000 draws from each chain after the warm-up stage. This takes about 2.5 seconds to run per model and leaves 30,000 post-warmup samples for subsequent use. While this is well in excess of what is required for reliable estimation, the large number of post-warmup draws is needed to ensure stability of the bridge sampling approximation for the marginal likelihood in the next stage.</p>
<pre class="r"><code># Full model with female rebels variable
logit_full_femrebels &lt;- bf(
  elect_party ~ rebel_gov_theta + nationalist_ideol + left_ideol + 
    idependence + ethnic_id + party_affil + africa + post_coldwar + 
    femrebels,
  center = FALSE
)

# Reduced model with female rebels variable
logit_minimal_femrebels &lt;- bf(
  elect_party ~ rebel_gov_theta + africa + femrebels,
  center = FALSE
)

## Bayesian Model Formulas----
logit_forms &lt;- list(
  logit_full_femrebels,
  logit_full_femrebels,
  logit_minimal_femrebels,
  logit_minimal_femrebels
)

## Data corresponding to each model----
data_list &lt;- list(
  model_df_a,
  model_df_b,
  model_df_a,
  model_df_b
)

## Specify file paths for the main models----
femrebels_model_files &lt;- c(
  str_c(base_dir, &quot;assets/models/Logit_Model_M1_Main&quot;),
  str_c(base_dir, &quot;assets/models/Logit_Model_M2_Main_Exs&quot;),
  str_c(base_dir, &quot;assets/models/Logit_Model_M4_Main&quot;),
  str_c(base_dir, &quot;assets/models/Logit_Model_M5_Main_Exs&quot;)
)

## Create a list to store the main models in----
femrebels_mods &lt;- list()

## Fit each of the models in the formula list sequentially----
for (i in 1:length(logit_forms)) {
  femrebels_mods[[i]] &lt;- brm(
    logit_forms,
    data = data_list[[i]],
    family = bernoulli(link = &quot;logit&quot;),
    prior = prior(normal(0, 0.75), class = &quot;b&quot;) +
      prior(student_t(15, 0, 1), class = &quot;b&quot;, coef = &quot;Intercept&quot;),
    chains = 6,
    cores = 6L,
    warmup = 3000,
    iter = 8000,
    seed = 12345,
    backend = &quot;cmdstanr&quot;,
    sample_prior = &quot;yes&quot;,
    save_pars = save_pars(all = TRUE),
    refresh = 0,
    control = list(max_treedepth = 12),
    file = femrebels_model_files[i]
  )
}</code></pre>
</div>
<div id="post-estimation" class="section level3">
<h3>Post-Estimation</h3>
<p>To obtain the posterior model probabilities, we first need to estimate the log marginal likelihood. <code>brms</code> provides an interface to the <code>bridgesampler</code> package which can be called by passing the <code>criterion = "marglik"</code> argument to the <code>add_criterion</code> function as shown below which will automatically save the result to the fitted model object. Given the relatively small number of observations, exact leave one out cross-validation (LOO-CV) is in the realm of computational tractability so I rely on that for the stacking and pseudo-BMA+ weighting approaches. In cases where exact LOO-CV is not computationally tractable, one can rely on importance sampling-based approximations.</p>
<pre class="r"><code>## Add log marginal likelihood----
for (i in 1:length(femrebels_mods)) {
  femrebels_mods[[i]] &lt;- add_criterion(
    femrebels_mods[[i]],
    criterion = &quot;marglik&quot;,
    seed = 12345,
    max_iter = 5e4,
    repetitions = 500,
    cores = 12
  )
}

## Use future to parallelize k-fold validation----
plan(multisession(workers = 6))

## Perform exact leave-one-out cross validation----
for (i in 1:length(femrebels_mods)) {
  femrebels_mods[[i]] &lt;- add_criterion(
    femrebels_mods[[i]],
    criterion = &quot;kfold&quot;,
    folds = &quot;loo&quot;,
    save_fits = TRUE,
    sample_prior = &quot;no&quot;,
    chains = 4,
    seed = 12345,
    ndraws = 3000,
    cores = 4L
  )
}</code></pre>
</div>
<div id="bma" class="section level3">
<h3>BMA</h3>
<p>To obtain the posterior probability weights we simply need to pass each of the stored <code>marglik</code> objects for the models containing the predictor we want to calculate a model averaged AME for to the <code>post_prob</code> function from the <code>bridgesampling</code> package. We’ll assume here that each of the full models is slightly more likely a priori than the reduced form versions. The resulting <code>postprob_weights</code> object is a 500 <span class="math inline">\(\times\)</span> 4 matrix of posterior model probability weights.</p>
<pre class="r"><code>## Create a matrix of posterior probabilities----
postprob_weights &lt;- bridgesampling::post_prob(
  femrebels_mods[[1]]$criteria$marglik, # Full Model, Including Suicide Bombers
  femrebels_mods[[2]]$criteria$marglik, # Full Model, Excluding Suicide Bombers
  femrebels_mods[[3]]$criteria$marglik, # Reduced Model, Including Suicide Bombers
  femrebels_mods[[4]]$criteria$marglik,  # Reduced Model, Excluding Suicide Bombers
  prior_prob = c(0.3, 0.3, 0.2, 0.2),
  model_names = c(&quot;post_prob_a&quot;, &quot;post_prob_b&quot;, &quot;post_prob_c&quot;, &quot;post_prob_d&quot;)
)</code></pre>
<p>In theory, it would be ideal to propagate any additional uncertainty introduced by the bridge sampling approximation forward by calculating the distribution of marginal effects for each of the 500 rows of the weights matrix. In practice, this proves computationally intractable so we instead take the average posterior probability of each model as shown below. While I focus on comparing contrasts via <code>comparisons</code> here, the same general ideas and workflow can be applied with <code>marginaleffects</code>.</p>
<pre class="r"><code>## Calculate Bayesian model averaged contrasts
bma_contrasts_femrebels &lt;- comparisons(
    femrebels_mods[[2]],
    m1 = femrebels_mods[[3]],
    m2 = femrebels_mods[[5]],
    m3 = femrebels_mods[[6]],
    variables = &quot;femrebels&quot;,
    contrast_numeric = &quot;minmax&quot;,
    type = &quot;average&quot;,
    weights = apply(postprob_weights, 2, mean),
    method = &quot;posterior_epred&quot;
  )

## Extract the model averaged posterior draws
bma_contrasts_draws &lt;- posteriordraws(bma_contrasts_femrebels)

## Print the output of comparisons
summary(bma_contrasts_femrebels)</code></pre>
<pre><code>## Average contrasts 
##                 Group      Term  Contrast Effect   2.5 % 97.5 %
## 1 main_marginaleffect femrebels Max - Min 0.1882 0.03266 0.3397
## 
## Model type:  brmsfit 
## Prediction type:  average</code></pre>
<p>Combining the models based on their posterior probability weights, we see from the output of the <code>summary</code> function that rebel groups in which women participated as combatants during wartime compared to those groups that did not feature women in combat roles were more likely to transition to post-conflict political parties with an average difference of 0.1882 [CI: 0.03266, 0.3397]. We can also plot the resulting <code>bma_contrasts_draws</code> object using <code>{tidybayes}</code> to get a better look at the full model averaged posterior.</p>
<pre class="r"><code># Plot the model average posterior distribution of the contrasts
ggplot(data = bma_contrasts_draws, aes(x = draw)) +
  # Add a slab interval geom
  stat_slabinterval(
    aes(slab_alpha = stat(pdf), shape = term, fill = term),
    fill_type = &quot;gradient&quot;,
    point_interval = median_qi,
    show.legend = F,
    .width = c(0.68, 0.89)
  ) +
  # Set the fill color
  scale_fill_manual(values = &quot;#208820&quot;) +
  # Set the shape parameter
  scale_shape_manual(values = 24) +
  # x axis labeling tweaks
  scale_x_continuous(breaks = scales::pretty_breaks(n = 8)) +
  # y axis labeling tweaks
  scale_y_continuous(breaks = scales::pretty_breaks(n = 5)) +
  # Apply theme settings
  plot_theme(
    plot.margin = margin(2, 5, 4, 2, &quot;mm&quot;),
    xaxis_size = 22
    ) +
  # Add labels to the plot
  labs(
    x = latex2exp::TeX(r&#39;($E\[Pr(Party \, Transition \, | \, Female \, Combatants$) - Pr(Party \, Transition \, | \, No \, Female \, Combatants$)\])&#39;),
    y = &quot;&quot;,
    title = &quot;Figure 2. Bayesian Model Averaged Marginal Contrasts&quot;
  )</code></pre>
<p><img src="assets/figures/Figure_2_BAMEs_Female_Combatants.jpeg" width="100%" height="100%" style="display: block; margin: auto;" /></p>
</div>
<div id="pseudo-bma-and-stacking" class="section level3">
<h3>Pseudo-BMA+ and Stacking</h3>
<p>Turning to the alternatives to traditional BMA proposed by <span class="citation">Yao et al. (<a href="#ref-Yao2018" role="doc-biblioref">2018</a>)</span>, we begin by calculating the pointwise log likelihood and relative efficiency for each model in the set under consideration as shown in the documentation for the <code>loo</code> package <span class="citation">(<a href="#ref-Vehtari2020" role="doc-biblioref">Vehtari et al. 2020</a>)</span>.</p>
<pre class="r"><code># Calculate the log likelihood for each model
femrebels_log_lik_list &lt;- map(
  .x = femrebels_mods,
  ~ log_lik(.x, cores = 8L)
)

# Relative EFF
femrebels_r_eff_list &lt;- map(
  .x = femrebels_log_lik_list,
  ~ loo::relative_eff(exp(.x), chain_id = 1:30000)
)</code></pre>
<p>Once we have obtained the above quantities, we can calculate three alternative sets of weights and see how they compare to the posterior probability model weights.</p>
<pre class="r"><code>## Calculate PSIS-LOO Based Stacking Weights
femrebels_stacking_weights &lt;- loo_model_weights(
  femrebels_log_lik_list,
  method = &quot;stacking&quot;,
  r_eff_list = femrebels_r_eff_list,
  optim_control = list(reltol = 1e-10),
  cores = 8L
)

## Calculate model averaged contrasts based on stacking weights
stacking_contrasts_femrebels &lt;- comparisons(
  femrebels_mods[[1]],
  m1 = femrebels_mods[[2]],
  m2 = femrebels_mods[[3]],
  m3 = femrebels_mods[[4]],
  variables = &quot;femrebels&quot;,
  contrast_numeric = &quot;minmax&quot;,
  type = &quot;average&quot;,
  weights = femrebels_stacking_weights,
  method = &quot;posterior_epred&quot;
)

## Extract the stacked posterior draws
stacking_contrasts_draws &lt;- posteriordraws(stacking_contrasts_femrebels)

## Print the output of comparisons
summary(stacking_contrasts_femrebels)</code></pre>
<pre><code>## Average contrasts 
##                 Group      Term  Contrast Effect   2.5 % 97.5 %
## 1 main_marginaleffect femrebels Max - Min 0.1968 0.05126 0.3387
## 
## Model type:  brmsfit 
## Prediction type:  average</code></pre>
<pre class="r"><code>## Calculate Pseudo-BMA+ Weights
femrebels_pbma_weights &lt;- loo_model_weights(
  femrebels_log_lik_list,
  method = &quot;pseudobma&quot;,
  r_eff_list = femrebels_r_eff_list,
  BB = TRUE, # Pseudo-BMA+ with Bayesian Bootstrap
  BB_n = 10000, # Use 10000 replications for Bayesian Bootstrap
  cores = 8L
)

## Calculate Bayesian model averaged contrasts
pbma_contrasts_femrebels &lt;- comparisons(
  femrebels_mods[[1]],
  m1 = femrebels_mods[[2]],
  m2 = femrebels_mods[[3]],
  m3 = femrebels_mods[[4]],
  variables = &quot;femrebels&quot;,
  contrast_numeric = &quot;minmax&quot;,
  type = &quot;average&quot;,
  weights = femrebels_pbma_weights,
  method = &quot;posterior_epred&quot;
)

## Extract the stacked posterior draws
pbma_contrasts_draws &lt;- posteriordraws(pbma_contrasts_femrebels)

## Print the output of comparisons
summary(pbma_contrasts_femrebels)</code></pre>
<pre><code>## Average contrasts 
##                 Group      Term  Contrast Effect   2.5 % 97.5 %
## 1 main_marginaleffect femrebels Max - Min 0.1891 0.03513 0.3393
## 
## Model type:  brmsfit 
## Prediction type:  average</code></pre>
<pre class="r"><code># Calculate model weights based on exact LOO-CV
femrebels_kfold_weights &lt;- model_weights(
  femrebels_mods[[1]],
  femrebels_mods[[2]],
  femrebels_mods[[3]],
  femrebels_mods[[4]],
  method = &quot;kfold&quot;,
  cores = 8L
)

## Calculate model averaged contrasts based on exact LOO-CV weights
kfold_contrasts_femrebels &lt;- comparisons(
  femrebels_mods[[1]],
  m1 = femrebels_mods[[2]],
  m2 = femrebels_mods[[3]],
  m3 = femrebels_mods[[4]],
  variables = &quot;femrebels&quot;,
  contrast_numeric = &quot;minmax&quot;,
  type = &quot;average&quot;,
  weights = femrebels_kfold_weights,
  method = &quot;posterior_epred&quot;
)

## Extract the stacked posterior draws
kfold_contrasts_draws &lt;- posteriordraws(kfold_contrasts_femrebels)

## Print the output of comparisons
summary(kfold_contrasts_femrebels)</code></pre>
<pre><code>## Average contrasts 
##                 Group      Term  Contrast Effect   2.5 % 97.5 %
## 1 main_marginaleffect femrebels Max - Min 0.1968 0.05126 0.3387
## 
## Model type:  brmsfit 
## Prediction type:  average</code></pre>
<p>We see that each of these approaches gives more or less the same answer–I have yet to personally encounter a case involving real data in which stacking and BMA based approaches differ wildly in the answers they provide. To get a closer look of where the slight differences are coming from, we can look at the model weights derived from each method.</p>
<pre class="r"><code># Print the weights
tibble::tibble(
  model = 1:4,
  stacking_weights = femrebels_stacking_weights,
  pseudo_bma_weights = femrebels_pbma_weights,
  kfold_weights = femrebels_kfold_weights,
  postprob_weights = apply(femrebels_postprob_weights, 2, mean)
)
## # A tibble: 4 x 5
##   model stacking_weights pseudo_bma_weights kfold_weights postprob_weights
##   &lt;int&gt; &lt;stckng_w&gt;       &lt;psdbm_b_&gt;                 &lt;dbl&gt;            &lt;dbl&gt;
## 1     1 4.343148e-10     0.01708624          0.000000288            0.0267
## 2     2 2.980128e-09     0.04954048          0.0000000272           0.0713
## 3     3 8.537654e-09     0.22259930          0.00000615             0.237 
## 4     4 1.000000e+00     0.71077398          1.00                   0.665</code></pre>
<p>The output above is quite informative in highlighting where the differences lie–the two stacking-based approaches are selecting a single model with probability 1. This may be preferable under some circumstances and less so in others depending on the research question and inferential goals. Traditional BMA and pseudo-BMA with Bayesian Bootstrap produce virtually identical estimates for the weights.</p>
</div>
</div>
<div id="concluding-thoughts" class="section level2">
<h2>Concluding Thoughts</h2>
<p>This concludes this brief illustration of BMA with <code>{brms}</code> and <code>{marginaleffects}</code>, though there will almost certainly be subsequent additions and further posts on this topic. The general idea is that selecting a single model is usually the wrong approach and there is seldom any need to do so. By applying probability theory to average across posterior distributions when estimating average effects, we can obtain interpretable model averaged estimates. Finally, remember that <em>friends don’t let friends do stepwise regression</em>.</p>
</div>
<div id="references" class="section level2 unnumbered">
<h2>References</h2>
<div id="refs" class="references csl-bib-body hanging-indent">
<div id="ref-Achen2005" class="csl-entry">
Achen, Christopher H. 2005. <span>“Let’s Put Garbage-Can Regressions and Garbage-Can Probits Where They Belong.”</span> <em>Conflict Management and Peace Science</em> 22(4): 327–39.
</div>
<div id="ref-Albert2022" class="csl-entry">
Albert, Karen E. 2022. <span>“What Is Rebel Governance? Introducing a New Dataset on Rebel Institutions, 1945<span></span>2012.”</span> <em>Journal of Peace Research</em>.
</div>
<div id="ref-ArelBundock2022" class="csl-entry">
Arel-Bundock, Vincent. 2022. <em>Marginaleffects: Marginal Effects, Marginal Means, Predictions, and Contrasts</em>. <a href="https://vincentarelbundock.github.io/marginaleffects/">https://vincentarelbundock.github.io/marginaleffects/</a>.
</div>
<div id="ref-Bartels1997" class="csl-entry">
Bartels, Larry M. 1997. <span>“Specification Uncertainty and Model Averaging.”</span> <em>American Journal of Political Science</em> 41: 641–74.
</div>
<div id="ref-Braithwaite2019" class="csl-entry">
Braithwaite, Jessica Maves, and Kathleen Gallagher Cunningham. 2019. <span>“When Organizations Rebel: Introducing the Foundations of Rebel Group Emergence (<span>FORGE</span>) Dataset.”</span> <em>International Studies Quarterly</em> 64(1): 183–93.
</div>
<div id="ref-Buerkner2017" class="csl-entry">
Bürkner, Paul-Christian. 2017. <span>“<span class="nocase">brms</span>: An <span>R</span> Package for Bayesian Multilevel Models Using Stan.”</span> <em>Journal of Statistical Software</em> 80: 1–28.
</div>
<div id="ref-Buerkner2018" class="csl-entry">
———. 2018. <span>“Advanced Bayesian Multilevel Modeling with the <span>R</span> Package Brms.”</span> <em>The R Journal</em> 10: 395–411.
</div>
<div id="ref-Clarke2005" class="csl-entry">
Clarke, Kevin A. 2005. <span>“The Phantom Menace: Omitted Variable Bias in Econometric Research.”</span> <em>Conflict Management and Peace Science</em> 22(4): 341–52.
</div>
<div id="ref-Clarke2007" class="csl-entry">
———. 2007. <span>“The Necessity of Being Comparative: Theory Confirmation in Quantitative Political Science.”</span> <em>Comparative Political Studies</em> 40: 886–908.
</div>
<div id="ref-Clarke2009" class="csl-entry">
———. 2009. <span>“Return of the Phantom Menace.”</span> <em>Conflict Management and Peace Science</em> 26(1): 46–66.
</div>
<div id="ref-Cranmer2017" class="csl-entry">
Cranmer, Skyler J., and Bruce A. Desmarais. 2017. <span>“What Can We Learn from Predictive Modeling?”</span> <em>Political Analysis</em> 25(2): 145–66.
</div>
<div id="ref-Cranmer2015" class="csl-entry">
Cranmer, Skyler J., Douglas R. Rice, and Randolph M. Siverson. 2015. <span>“What to Do about Atheoretic Lags.”</span> <em>Political Science Research and Methods</em> 5(4): 641–65.
</div>
<div id="ref-Gelman1998" class="csl-entry">
Gelman, Andrew, and Xiao-Li Meng. 1998. <span>“Simulating Normalizing Constants: From Importance Sampling to Bridge Sampling to Path Sampling.”</span> <em>Statistical Science</em> 13(2): 163–85.
</div>
<div id="ref-Gronau2017" class="csl-entry">
Gronau, Quentin F. et al. 2017. <span>“A Tutorial on Bridge Sampling.”</span> <em>Journal of Mathematical Psychology</em> 81: 80–97.
</div>
<div id="ref-Hollenbach2020" class="csl-entry">
Hollenbach, Florian M., and Jacob M. Montgomery. 2020. <span>“Bayesian Model Selection, Model Comparison, and Model Averaging.”</span> In <em>The <span>SAGE</span> Handbook of Research Methods in Political Science and International Relations</em>, eds. Luigi Curini and Robert Franzese. <span>SAGE</span> Publications Ltd, 937–60.
</div>
<div id="ref-Imai2011" class="csl-entry">
Imai, Kosuke, and Dustin Tingley. 2011. <span>“A Statistical Method for Empirical Testing of Competing Theories.”</span> <em>American Journal of Political Science</em> 56(1): 218–36.
</div>
<div id="ref-Juhl2019" class="csl-entry">
Juhl, Sebastian. 2019. <span>“The Sensitivity of Spatial Regression Models to Network Misspecification.”</span> <em>Political Analysis</em> 28(1): 1–19.
</div>
<div id="ref-McShane2019" class="csl-entry">
McShane, Blakeley B. et al. 2019. <span>“Abandon Statistical Significance.”</span> <em>The American Statistician</em> 73(SUP1): 235–45.
</div>
<div id="ref-Montgomery2010" class="csl-entry">
Montgomery, Jacob M., and Brendan Nyhan. 2010. <span>“Bayesian Model Averaging: Theoretical Developments and Practical Applications.”</span> <em>Political Analysis</em> 18(2): 245–70.
</div>
<div id="ref-Montgomery2018" class="csl-entry">
Montgomery, Jacob M., Brendan Nyhan, and Michelle Torres. 2018. <span>“How Conditioning on Posttreatment Variables Can Ruin Your Experiment and What to Do about It.”</span> <em>American Journal of Political Science</em> 62(3): 760–75.
</div>
<div id="ref-Nafa2022" class="csl-entry">
Nafa, A. Jordan, and P. DeAnne Roark. 2022. <span>“Broadening the Base? How Female Rebels Impact the Transition and Survival of Post-Conflict Rebel Parties.”</span>
</div>
<div id="ref-Piironen2016" class="csl-entry">
Piironen, Juho, and Aki Vehtari. 2016. <span>“Comparison of Bayesian Predictive Methods for Model Selection.”</span> <em>Statistics and Computing</em> 27: 711–35.
</div>
<div id="ref-Pluemper2018" class="csl-entry">
Plümper, Thomas, and Richard Traunmüller. 2018. <span>“The Sensitivity of Sensitivity Analysis.”</span> <em>Political Science Research and Methods</em> 8(1): 149–59.
</div>
<div id="ref-Schad2022" class="csl-entry">
Schad, Daniel J. et al. 2022. <span>“Workflow Techniques for the Robust Use of Bayes Factors.”</span> <em>Psychological Methods</em>.
</div>
<div id="ref-Schrodt2014" class="csl-entry">
Schrodt, Philip A. 2014. <span>“Seven Deadly Sins of Contemporary Quantitative Political Analysis.”</span> <em>Journal of Peace Research</em> 51: 287–300.
</div>
<div id="ref-Vehtari2020" class="csl-entry">
Vehtari, Aki et al. 2020. <span>“Loo: Efficient Leave-One-Out Cross-Validation and WAIC for Bayesian Models.”</span> <a href="https://mc-stan.org/loo/">https://mc-stan.org/loo/</a>.
</div>
<div id="ref-Wang2020" class="csl-entry">
Wang, Lazhi, David E. Jones, and Xiao-Li Meng. 2020. <span>“Warp Bridge Sampling: The Next Generation.”</span> <em>Journal of the American Statistical Association</em>: 1–17.
</div>
<div id="ref-Ward2010" class="csl-entry">
Ward, Michael D, Brian D Greenhill, and Kristin M Bakke. 2010. <span>“The Perils of Policy by p-Value: Predicting Civil Conflicts.”</span> <em>Journal of Peace Research</em> 47(4): 363–75.
</div>
<div id="ref-Wood2017" class="csl-entry">
Wood, Reed M., and Jakana L. Thomas. 2017. <span>“Women on the Frontline.”</span> <em>Journal of Peace Research</em> 54(1): 31–46.
</div>
<div id="ref-Yao2018" class="csl-entry">
Yao, Yuling, Aki Vehtari, Daniel Simpson, and Andrew Gelman. 2018. <span>“Using Stacking to Average Bayesian Predictive Distributions (with Discussion).”</span> <em>Bayesian Analysis</em> 13(3): 917–1007.
</div>
</div>
</div>
<div class="footnotes">
<hr />
<ol>
<li id="fn1"><p>This focus on only the first post-conflict election is necessary to limit the universe of possible confounders and avoid issues of collider bias that arise when examining subsequent elections.<a href="#fnref1" class="footnote-back">↩︎</a></p></li>
<li id="fn2"><p>I construct a latent measure of the degree to which rebel groups engage in functions characteristic of state institutions during wartime based on estimates from a Bayesian multidimensional item response model.<a href="#fnref2" class="footnote-back">↩︎</a></p></li>
</ol>
</div>
